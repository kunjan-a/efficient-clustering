\chapter{Introduction}
\section{Introduction}

\emph{ ``Understanding our world requires conceptualizing the similarities and differences between the entities that compose it'' } \cite{tryon}.
For understanding phenomena in real life there is always a need to aggregate all the raw data and then perform analysis over it. Various methodologies have been adopted for analyzing the raw data both for performing supervised and unsupervised learning. Clustering is one such technique, an unsupervised classification or exploratory data analysis without availability of any labeled data \cite{Xu:2009}. It aids in organizing the data instances into an efficient representation that characterizes the data being sampled.

Formally, clustering groups data instances into subsets in such a manner that similar instances are grouped together, while different instances belong to different groups \cite{lior}. The goal of clustering is to separate a finite, unlabeled data set into a finite and discrete set of ``natural'', hidden data structures, rather than to provide an accurate characterization of unobserved samples generated from the same probability distribution \cite{baraldi,mulier}.

There are a large number of clustering algorithms and heuristics that have been developed over the last many decades \cite{Jain:1999,lior,Xu:2009}. Since these clustering algorithms have to analyze real world data, they often have to deal with huge datasets that require high processing power. Over the last few years, Graphics Processing Units (GPUs) have emerged as a new alternative for handling operations with high levels of parallelism \cite{kirk}. Also since the data operations in clustering algorithms are largely independent and compute-intensive, the large number of cores available on GPUs offer a more natural alternative for extracting maximum parallelism and efficiency \cite{che_et_al}.

In our work, we will be looking at design and implementation of K-means clustering algorithm on GPUs using a general-purpose parallel programming model, namely Compute Unified Device Architecture (CUDA) \cite{cuda}. We analyze the scalability of our proposed methods with increase in number and dimensionality of data points as well as the number of clusters. We also compare our results with current best available implementations on GPUs and 24-way threaded parallel CPU implementations.

\section{Related work}

The K-means algorithm was independently proposed by Hugo Steinhaus in 1956 \cite{hugo}, Stuart Lloyd in 1957 \cite{lloyd}, Ball \& Hall in 1965 \cite{ball_hall} and James MacQueen in 1967 \cite{macqueen}.
Because of its efficiency in clustering large data sets it is one of the most popular and commonly used partitional algorithm \cite{jain:2009}.
It has been successfully used for analysis in variety of different fields like economics (market segmentation), life and medical sciences (genetics, microbiology), engineering (machine learning), astronomy, earth sciences and sociology (behavior pattern discovery) \cite{Xu:2009}. 

Initial implementations of K-means clustering algorithm on GPUs have been reported by Takizwa et al \cite{takizawa} and Cao et al \cite{cao} in 2006. Ma et al \cite{ma} made a generic translation system to port CPU code over GPUs and were able to get 20x speedup over original sequential CPU implementation. In 2007, a team at University of Virgina implemented K-means on G80 GPU and reported 8x speedup in comparison to a single threaded Pentium 4 processor \cite{che2007}. Later in 2008, they revised their work and were able to achieve 35x speedup on GTX 260 GPU over a dual core CPU \cite{che_et_al}. In 2008, another team at Hong Kong University of Science and Technology introduced GPUMiner suite \cite{gpuminer}, which contained parallel data mining implementations on graphics processors. They reported 5x improvement over the first work \cite{che2007} done by University of Virgina but it was still slower than their later implementation \cite{che_et_al}.

In 2009, a team of researchers at HP labs further improved these results \cite{wu_hp}. They achieved upto 4x speedup over university of Virgina's implementation \cite{che_et_al} and 20x to 70x speedup over GPUMiner \cite{gpuminer}. This work was further improved by Li et al \cite{li_et_al} in 2010 by making separate K-means implementations for low dimensional and higher dimensional input data sets. In 2011, Wasif et al \cite{wasif_et_al} reported 2x improvement over work of Li et al \cite{li_et_al}. They have also reported 70x speedup on Fermi architecture based GTX480 GPU over a 32 bit Intel Core 2 duo processor.

\section{Outline}
The rest of the thesis is organized as follows: In Chapter 2, we give an introduction to K-means clustering algorithm. We illustrate its different stages and then analyze its complexity and scalability. Also, we take a look into NU-MineBench benchmark suite \cite{numine} which we have used for our CPU implementations. Chapter 3 gives an overview of GPU architecture of NVIDIA's Tesla and Fermi graphics processing units. It also provides a short introduction to Compute Unified Device Architecture (CUDA) API which we have used to implement K-means on NVIDIA GPUs. Chapters 4 and 5 present details of our implementation of K-means algorithm. In Chapter 6 we show comparison of our approach with the current published results. Also, we present comparison of our results on NVIDIA GPUs with 24-way threaded parallel CPU implementations for real world datasets. Finally, Chapter 7 concludes our work and presents scope for future research.